<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>dro.src.linear_model.Wasserstein_DRO &mdash; dro 0.1.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css" />

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/sphinx_highlight.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            dro
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/data.html">Data Modules</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/data.html#dro.src.data.dataloader_classification.classification_basic"><code class="docutils literal notranslate"><span class="pre">classification_basic()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/data.html#dro.src.data.dataloader_classification.classification_DN21"><code class="docutils literal notranslate"><span class="pre">classification_DN21()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/data.html#dro.src.data.dataloader_classification.classification_SNVD20"><code class="docutils literal notranslate"><span class="pre">classification_SNVD20()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/data.html#dro.src.data.dataloader_classification.classification_LWLC"><code class="docutils literal notranslate"><span class="pre">classification_LWLC()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/data.html#dro.src.data.dataloader_regression.regression_basic"><code class="docutils literal notranslate"><span class="pre">regression_basic()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/data.html#dro.src.data.dataloader_regression.regression_DN20_1"><code class="docutils literal notranslate"><span class="pre">regression_DN20_1()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/data.html#dro.src.data.dataloader_regression.regression_DN20_2"><code class="docutils literal notranslate"><span class="pre">regression_DN20_2()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/data.html#dro.src.data.dataloader_regression.regression_DN20_3"><code class="docutils literal notranslate"><span class="pre">regression_DN20_3()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/data.html#dro.src.data.dataloader_regression.regression_LWLC"><code class="docutils literal notranslate"><span class="pre">regression_LWLC()</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/linear_model.html">Linear DRO Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.chi2_dro.Chi2DROError"><code class="docutils literal notranslate"><span class="pre">Chi2DROError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.chi2_dro.Chi2DRO"><code class="docutils literal notranslate"><span class="pre">Chi2DRO</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.chi2_dro.Chi2DRO.update"><code class="docutils literal notranslate"><span class="pre">Chi2DRO.update()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.chi2_dro.Chi2DRO.fit"><code class="docutils literal notranslate"><span class="pre">Chi2DRO.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.chi2_dro.Chi2DRO.evaluate"><code class="docutils literal notranslate"><span class="pre">Chi2DRO.evaluate()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.chi2_dro.Chi2DRO.worst_distribution"><code class="docutils literal notranslate"><span class="pre">Chi2DRO.worst_distribution()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.cvar_dro.CVaRDROError"><code class="docutils literal notranslate"><span class="pre">CVaRDROError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.cvar_dro.CVaRDRO"><code class="docutils literal notranslate"><span class="pre">CVaRDRO</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.cvar_dro.CVaRDRO.update"><code class="docutils literal notranslate"><span class="pre">CVaRDRO.update()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.cvar_dro.CVaRDRO.fit"><code class="docutils literal notranslate"><span class="pre">CVaRDRO.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.cvar_dro.CVaRDRO.worst_distribution"><code class="docutils literal notranslate"><span class="pre">CVaRDRO.worst_distribution()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.kl_dro.KLDROError"><code class="docutils literal notranslate"><span class="pre">KLDROError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.kl_dro.KLDRO"><code class="docutils literal notranslate"><span class="pre">KLDRO</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.kl_dro.KLDRO.update"><code class="docutils literal notranslate"><span class="pre">KLDRO.update()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.kl_dro.KLDRO.fit"><code class="docutils literal notranslate"><span class="pre">KLDRO.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.kl_dro.KLDRO.worst_distribution"><code class="docutils literal notranslate"><span class="pre">KLDRO.worst_distribution()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.tv_dro.TVDROError"><code class="docutils literal notranslate"><span class="pre">TVDROError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.tv_dro.TVDRO"><code class="docutils literal notranslate"><span class="pre">TVDRO</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.tv_dro.TVDRO.update"><code class="docutils literal notranslate"><span class="pre">TVDRO.update()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.tv_dro.TVDRO.fit"><code class="docutils literal notranslate"><span class="pre">TVDRO.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.tv_dro.TVDRO.worst_distribution"><code class="docutils literal notranslate"><span class="pre">TVDRO.worst_distribution()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.conditional_dro.ConditionalCVaRDROError"><code class="docutils literal notranslate"><span class="pre">ConditionalCVaRDROError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.conditional_dro.ConditionalCVaRDRO"><code class="docutils literal notranslate"><span class="pre">ConditionalCVaRDRO</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.conditional_dro.ConditionalCVaRDRO.update"><code class="docutils literal notranslate"><span class="pre">ConditionalCVaRDRO.update()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.conditional_dro.ConditionalCVaRDRO.fit"><code class="docutils literal notranslate"><span class="pre">ConditionalCVaRDRO.fit()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.hr_dro.HRDROError"><code class="docutils literal notranslate"><span class="pre">HRDROError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.hr_dro.HR_DRO_LR"><code class="docutils literal notranslate"><span class="pre">HR_DRO_LR</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.hr_dro.HR_DRO_LR.update"><code class="docutils literal notranslate"><span class="pre">HR_DRO_LR.update()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.hr_dro.HR_DRO_LR.fit"><code class="docutils literal notranslate"><span class="pre">HR_DRO_LR.fit()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.marginal_dro.MarginalCVaRDROError"><code class="docutils literal notranslate"><span class="pre">MarginalCVaRDROError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.marginal_dro.MarginalCVaRDRO"><code class="docutils literal notranslate"><span class="pre">MarginalCVaRDRO</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.marginal_dro.MarginalCVaRDRO.update"><code class="docutils literal notranslate"><span class="pre">MarginalCVaRDRO.update()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.marginal_dro.MarginalCVaRDRO.fit"><code class="docutils literal notranslate"><span class="pre">MarginalCVaRDRO.fit()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.MOT_DRO.MOTDROError"><code class="docutils literal notranslate"><span class="pre">MOTDROError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.MOT_DRO.MOTDRO"><code class="docutils literal notranslate"><span class="pre">MOTDRO</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.MOT_DRO.MOTDRO.update"><code class="docutils literal notranslate"><span class="pre">MOTDRO.update()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.MOT_DRO.MOTDRO.fit"><code class="docutils literal notranslate"><span class="pre">MOTDRO.fit()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.OR_Wasserstein_DRO.ORWDROError"><code class="docutils literal notranslate"><span class="pre">ORWDROError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.OR_Wasserstein_DRO.OR_Wasserstein_DRO"><code class="docutils literal notranslate"><span class="pre">OR_Wasserstein_DRO</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.OR_Wasserstein_DRO.OR_Wasserstein_DRO.update"><code class="docutils literal notranslate"><span class="pre">OR_Wasserstein_DRO.update()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.OR_Wasserstein_DRO.OR_Wasserstein_DRO.fit"><code class="docutils literal notranslate"><span class="pre">OR_Wasserstein_DRO.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.OR_Wasserstein_DRO.OR_Wasserstein_DRO.cheap_robust_mean_estimate"><code class="docutils literal notranslate"><span class="pre">OR_Wasserstein_DRO.cheap_robust_mean_estimate()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.OR_Wasserstein_DRO.OR_Wasserstein_DRO.worst_distribution"><code class="docutils literal notranslate"><span class="pre">OR_Wasserstein_DRO.worst_distribution()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.sinkhorn_dro.SinkhornDROError"><code class="docutils literal notranslate"><span class="pre">SinkhornDROError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.sinkhorn_dro.LinearModel"><code class="docutils literal notranslate"><span class="pre">LinearModel</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.sinkhorn_dro.LinearModel.forward"><code class="docutils literal notranslate"><span class="pre">LinearModel.forward()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.sinkhorn_dro.LinearModel.training"><code class="docutils literal notranslate"><span class="pre">LinearModel.training</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.sinkhorn_dro.SinkhornLinearDRO"><code class="docutils literal notranslate"><span class="pre">SinkhornLinearDRO</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.sinkhorn_dro.SinkhornLinearDRO.update"><code class="docutils literal notranslate"><span class="pre">SinkhornLinearDRO.update()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.sinkhorn_dro.SinkhornLinearDRO.predict"><code class="docutils literal notranslate"><span class="pre">SinkhornLinearDRO.predict()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.sinkhorn_dro.SinkhornLinearDRO.score"><code class="docutils literal notranslate"><span class="pre">SinkhornLinearDRO.score()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.sinkhorn_dro.SinkhornLinearDRO.fit"><code class="docutils literal notranslate"><span class="pre">SinkhornLinearDRO.fit()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDROError"><code class="docutils literal notranslate"><span class="pre">WassersteinDROError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDRO"><code class="docutils literal notranslate"><span class="pre">WassersteinDRO</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDRO.update"><code class="docutils literal notranslate"><span class="pre">WassersteinDRO.update()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDRO.penalization"><code class="docutils literal notranslate"><span class="pre">WassersteinDRO.penalization()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDRO.fit"><code class="docutils literal notranslate"><span class="pre">WassersteinDRO.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDRO.distance_compute"><code class="docutils literal notranslate"><span class="pre">WassersteinDRO.distance_compute()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDRO.lipschitz_norm"><code class="docutils literal notranslate"><span class="pre">WassersteinDRO.lipschitz_norm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDRO.worst_distribution"><code class="docutils literal notranslate"><span class="pre">WassersteinDRO.worst_distribution()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDROSatisificingError"><code class="docutils literal notranslate"><span class="pre">WassersteinDROSatisificingError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing"><code class="docutils literal notranslate"><span class="pre">Wasserstein_DRO_satisficing</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.input_dim"><code class="docutils literal notranslate"><span class="pre">Wasserstein_DRO_satisficing.input_dim</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.model_type"><code class="docutils literal notranslate"><span class="pre">Wasserstein_DRO_satisficing.model_type</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.fit_intercept"><code class="docutils literal notranslate"><span class="pre">Wasserstein_DRO_satisficing.fit_intercept</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.solver"><code class="docutils literal notranslate"><span class="pre">Wasserstein_DRO_satisficing.solver</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.update"><code class="docutils literal notranslate"><span class="pre">Wasserstein_DRO_satisficing.update()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.fit"><code class="docutils literal notranslate"><span class="pre">Wasserstein_DRO_satisficing.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.fit_depreciate"><code class="docutils literal notranslate"><span class="pre">Wasserstein_DRO_satisficing.fit_depreciate()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.penalization"><code class="docutils literal notranslate"><span class="pre">Wasserstein_DRO_satisficing.penalization()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.fit_oracle"><code class="docutils literal notranslate"><span class="pre">Wasserstein_DRO_satisficing.fit_oracle()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.worst_distribution"><code class="docutils literal notranslate"><span class="pre">Wasserstein_DRO_satisficing.worst_distribution()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/neural_model.html">Neural Network DRO Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.base_nn.DROError"><code class="docutils literal notranslate"><span class="pre">DROError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.base_nn.ParameterError"><code class="docutils literal notranslate"><span class="pre">ParameterError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.base_nn.DataValidationError"><code class="docutils literal notranslate"><span class="pre">DataValidationError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.base_nn.Linear"><code class="docutils literal notranslate"><span class="pre">Linear</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.base_nn.Linear.forward"><code class="docutils literal notranslate"><span class="pre">Linear.forward()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.base_nn.Linear.training"><code class="docutils literal notranslate"><span class="pre">Linear.training</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.base_nn.MLP"><code class="docutils literal notranslate"><span class="pre">MLP</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.base_nn.MLP.forward"><code class="docutils literal notranslate"><span class="pre">MLP.forward()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.base_nn.MLP.training"><code class="docutils literal notranslate"><span class="pre">MLP.training</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.base_nn.BaseNNDRO"><code class="docutils literal notranslate"><span class="pre">BaseNNDRO</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.base_nn.BaseNNDRO.criterion"><code class="docutils literal notranslate"><span class="pre">BaseNNDRO.criterion()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.base_nn.BaseNNDRO.fit"><code class="docutils literal notranslate"><span class="pre">BaseNNDRO.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.base_nn.BaseNNDRO.predict"><code class="docutils literal notranslate"><span class="pre">BaseNNDRO.predict()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.base_nn.BaseNNDRO.score"><code class="docutils literal notranslate"><span class="pre">BaseNNDRO.score()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.base_nn.BaseNNDRO.f1score"><code class="docutils literal notranslate"><span class="pre">BaseNNDRO.f1score()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.fdro_nn.Chi2NNDRO"><code class="docutils literal notranslate"><span class="pre">Chi2NNDRO</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.fdro_nn.Chi2NNDRO.criterion"><code class="docutils literal notranslate"><span class="pre">Chi2NNDRO.criterion()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.fdro_nn.CVaRNNDRO"><code class="docutils literal notranslate"><span class="pre">CVaRNNDRO</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.fdro_nn.CVaRNNDRO.criterion"><code class="docutils literal notranslate"><span class="pre">CVaRNNDRO.criterion()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.hrdro_nn.HRNNDRO"><code class="docutils literal notranslate"><span class="pre">HRNNDRO</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.hrdro_nn.HRNNDRO.criterion"><code class="docutils literal notranslate"><span class="pre">HRNNDRO.criterion()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.hrdro_nn.HRNNDRO.fit"><code class="docutils literal notranslate"><span class="pre">HRNNDRO.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.hrdro_nn.HRNNDRO.predict"><code class="docutils literal notranslate"><span class="pre">HRNNDRO.predict()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.wdro_nn.WNNDRO"><code class="docutils literal notranslate"><span class="pre">WNNDRO</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/neural_model.html#dro.src.neural_model.wdro_nn.WNNDRO.criterion"><code class="docutils literal notranslate"><span class="pre">WNNDRO.criterion()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">dro</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">dro.src.linear_model.Wasserstein_DRO</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for dro.src.linear_model.Wasserstein_DRO</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">dro.src.linear_model.base</span> <span class="kn">import</span> <span class="n">BaseLinearDRO</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">cvxpy</span> <span class="k">as</span> <span class="nn">cp</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">sqrtm</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<div class="viewcode-block" id="WassersteinDROError"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDROError">[docs]</a><span class="k">class</span> <span class="nc">WassersteinDROError</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base exception class for errors in Wasserstein DRO model.&quot;&quot;&quot;</span>
    <span class="k">pass</span></div>


<div class="viewcode-block" id="WassersteinDRO"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDRO">[docs]</a><span class="k">class</span> <span class="nc">WassersteinDRO</span><span class="p">(</span><span class="n">BaseLinearDRO</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wasserstein Distributionally Robust Optimization (WDRO) model</span>
<span class="sd">    </span>
<span class="sd">    This model minimizes a Wasserstein-robust loss function for both regression and classification.</span>

<span class="sd">    The Wasserstein distance is defined as the minimum probability coupling of two distributions for the distance metric: </span>
<span class="sd">        d((X_1, Y_1), (X_2, Y_2)) = (abs(cost_matrix^{1/2} @ (X_1 - X_2))_p)^{square} + kappa abs(Y_1 - Y_2).</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        input_dim (int): Dimensionality of the input features.</span>
<span class="sd">        model_type (str, default = &#39;svm&#39;): Model type indicator (&#39;svm&#39; for SVM, &#39;logistic&#39; for Logistic Regression, &#39;ols&#39; for Linear Regression for OLS, &#39;lad&#39; for Linear Regression for LAD).</span>
<span class="sd">        fit_intercept (bool, default = True): Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).</span>
<span class="sd">        solver (str, default = &#39;MOSEK&#39;): Optimization solver to solve the problem, default = &#39;MOSEK&#39;.</span>
<span class="sd">        eps (float): Robustness parameter for DRO.</span>
<span class="sd">        cost matrix (np.ndarray): the feature importance perturbation matrix with the dimension being (input_dim, input_dim).</span>
<span class="sd">        p (float or &#39;inf&#39;): Norm parameter for controlling the perturbation moment of X.</span>
<span class="sd">        kappa (float or &#39;inf&#39;): Robustness parameter for the perturbation of Y. Note that if we set kappa to be large enough to approximately infinite, this is equivalent to saying that we do not allow changes in Y.</span>

<span class="sd">    Reference:</span>
<span class="sd">    [1] OLS: &lt;https://www.cambridge.org/core/journals/journal-of-applied-probability/article/robust-wasserstein-profile-inference-and-applications-to-machine-learning/4024D05DE4681E67334E45D039295527&gt;</span>
<span class="sd">    [2] LAD / SVM / Logistic: &lt;https://jmlr.org/papers/volume20/17-633/17-633.pdf&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>


    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">model_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;svm&#39;</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">solver</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;MOSEK&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            input_dim (int): Dimension of the input features.</span>
<span class="sd">            model_type (str): Type of model (&#39;svm&#39;, &#39;logistic&#39;, &#39;ols&#39;, &#39;lad&#39;).</span>
<span class="sd">            fit_intercept (bool, default = True): Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).</span>
<span class="sd">            solver (str, default = &#39;MOSEK&#39;): Optimization solver to solve the problem, default = &#39;MOSEK&#39;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">BaseLinearDRO</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">model_type</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="p">,</span> <span class="n">solver</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cost_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cost_inv_transform</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sqrtm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_matrix</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">=</span> <span class="s1">&#39;inf&#39;</span>
        

<div class="viewcode-block" id="WassersteinDRO.update"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDRO.update">[docs]</a>    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update the model configuration</span>
<span class="sd">        </span>
<span class="sd">        Args: </span>
<span class="sd">            config (Dict[str, Any]): Configuration dictionary containing &#39;cost_matrix&#39;, &#39;eps&#39;, &#39;p&#39;, &#39;kappa&#39; keys for robustness parameter.</span>
<span class="sd">        Raises:</span>
<span class="sd">            WassersteinDROError: If any of the configs does not fall into its domain.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s1">&#39;cost_matrix&#39;</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">cost_matrix</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;cost_matrix&#39;</span><span class="p">]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cost_matrix</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">or</span> <span class="n">cost_matrix</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">cost_matrix</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
                <span class="k">raise</span> <span class="n">WassersteinDROError</span><span class="p">(</span><span class="s2">&quot;Cost Adjust Matrix &#39;cost matrix&#39; must be a PD matrix&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cost_matrix</span> <span class="o">=</span> <span class="n">cost_matrix</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cost_inv_transform</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sqrtm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_matrix</span><span class="p">))</span>
        <span class="k">if</span> <span class="s1">&#39;eps&#39;</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">or</span> <span class="n">eps</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">WassersteinDROError</span><span class="p">(</span><span class="s2">&quot;Robustness parameter &#39;eps&#39; must be a non-negative float.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>
        <span class="k">if</span> <span class="s1">&#39;p&#39;</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;p&#39;</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">p</span> <span class="o">!=</span> <span class="s1">&#39;inf&#39;</span> <span class="ow">and</span> <span class="p">((</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="n">WassersteinDROError</span><span class="p">(</span><span class="s2">&quot;Norm parameter &#39;p&#39; must be float and larger than 1.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

        <span class="k">if</span> <span class="s1">&#39;kappa&#39;</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">kappa</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;kappa&#39;</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">kappa</span> <span class="o">!=</span> <span class="s1">&#39;inf&#39;</span> <span class="ow">and</span> <span class="p">((</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kappa</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)))</span> <span class="ow">or</span> <span class="n">kappa</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
                <span class="k">raise</span> <span class="n">WassersteinDROError</span><span class="p">(</span><span class="s2">&quot;Y-Robustness parameter &#39;kappa&#39; must be a non-negative float.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">kappa</span> <span class="o">!=</span> <span class="s1">&#39;inf&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;ols&#39;</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Wasserstein Distributionally Robust OLS does not support changes of Y in the ambiguity set&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">kappa</span><span class="p">)</span></div>

        
<div class="viewcode-block" id="WassersteinDRO.penalization"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDRO.penalization">[docs]</a>    <span class="k">def</span> <span class="nf">penalization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">cp</span><span class="o">.</span><span class="n">Expression</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Module for computing the regularization part in the standard Wasserstein DRO problem.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta (cp.Expression): Feature vector with shape (n_feature,).</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            Float: Regularization term part.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">dual_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">!=</span> <span class="s1">&#39;inf&#39;</span><span class="p">:</span>
            <span class="n">dual_norm</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dual_norm</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;ols&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">cp</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_inv_transform</span> <span class="o">@</span> <span class="n">theta</span><span class="p">,</span> <span class="n">dual_norm</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;svm&#39;</span><span class="p">,</span> <span class="s1">&#39;logistic&#39;</span><span class="p">]:</span>
            <span class="k">return</span> <span class="n">cp</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_inv_transform</span> <span class="o">@</span> <span class="n">theta</span><span class="p">,</span> <span class="n">dual_norm</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;lad&#39;</span><span class="p">:</span>
            <span class="c1"># the dual of the \|\theta, -1\|_dual_norm penalization</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">==</span> <span class="s1">&#39;inf&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">cp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_inv_transform</span> <span class="o">@</span> <span class="n">theta</span><span class="p">,</span> <span class="n">dual_norm</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">cp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_inv_transform</span> <span class="o">@</span> <span class="n">theta</span><span class="p">,</span> <span class="n">dual_norm</span><span class="p">),</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span><span class="p">)</span></div>




        
<div class="viewcode-block" id="WassersteinDRO.fit"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDRO.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Fit the model using CVXPY to solve the Wasserstein distributionally robust optimization problem.</span>

<span class="sd">        Args:</span>
<span class="sd">            X (np.ndarray): Input feature matrix with shape (n_samples, n_features).</span>
<span class="sd">            y (np.ndarray): Target vector with shape (n_samples,).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: Model parameters dictionary with &#39;theta&#39; key.</span>

<span class="sd">        Raises:</span>
<span class="sd">            WassersteinDROError: If the optimization problem fails to solve.        </span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">sample_size</span><span class="p">,</span> <span class="n">feature_size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="n">feature_size</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">WassersteinDROError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected input with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="si">}</span><span class="s2"> features, got </span><span class="si">{</span><span class="n">feature_size</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sample_size</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="n">WassersteinDROError</span><span class="p">(</span><span class="s2">&quot;Input X and target y must have the same number of samples.&quot;</span><span class="p">)</span>



        <span class="n">theta</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>


        <span class="n">lamb_da</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">()</span>
        <span class="n">cons</span> <span class="o">=</span> <span class="p">[</span><span class="n">lamb_da</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalization</span><span class="p">(</span><span class="n">theta</span><span class="p">)]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;ols&#39;</span><span class="p">:</span>
            <span class="n">final_loss</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">theta</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sample_size</span><span class="p">)</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">lamb_da</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;svm&#39;</span><span class="p">,</span> <span class="s1">&#39;logistic&#39;</span><span class="p">]:</span>
                <span class="n">s</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">sample_size</span><span class="p">)</span>
                <span class="n">cons</span> <span class="o">+=</span> <span class="p">[</span><span class="n">s</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cvx_loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">b</span><span class="p">)]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">!=</span> <span class="s1">&#39;inf&#39;</span><span class="p">:</span>
                    <span class="n">cons</span> <span class="o">+=</span> <span class="p">[</span><span class="n">s</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cvx_loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">-</span><span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">lamb_da</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span><span class="p">]</span>
                <span class="n">final_loss</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="n">sample_size</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">*</span> <span class="n">lamb_da</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># model type == &#39;lad&#39; for general p.</span>
                <span class="n">final_loss</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cvx_loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span> <span class="o">/</span> <span class="n">sample_size</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">*</span> <span class="n">lamb_da</span>

        <span class="n">problem</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Problem</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">Minimize</span><span class="p">(</span><span class="n">final_loss</span><span class="p">),</span> <span class="n">cons</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">problem</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">cp</span><span class="o">.</span><span class="n">error</span><span class="o">.</span><span class="n">SolverError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">WassersteinDROError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimization failed to solve using </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
        
        <span class="k">if</span> <span class="n">theta</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">WassersteinDROError</span><span class="p">(</span><span class="s2">&quot;Optimization did not converge to a solution.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">value</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">value</span>

        <span class="n">model_params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">model_params</span><span class="p">[</span><span class="s2">&quot;theta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">model_params</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
        <span class="k">return</span> <span class="n">model_params</span></div>
    
<div class="viewcode-block" id="WassersteinDRO.distance_compute"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDRO.distance_compute">[docs]</a>    <span class="k">def</span> <span class="nf">distance_compute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_1</span><span class="p">:</span> <span class="n">cp</span><span class="o">.</span><span class="n">Expression</span><span class="p">,</span> <span class="n">X_2</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y_1</span><span class="p">:</span> <span class="n">cp</span><span class="o">.</span><span class="n">Expression</span><span class="p">,</span> <span class="n">Y_2</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">cp</span><span class="o">.</span><span class="n">Expression</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computing the distance between two points (X_1, Y_1), (X_2, Y_2) under our defined metric in cvxpy problem</span>

<span class="sd">        Args:</span>
<span class="sd">            X_1 (cp.Expression): Input feature-1 (n_feature,);</span>
<span class="sd">            Y_1 (float): Input label-1;</span>
<span class="sd">            X_2 (cp.Expression): Input feature-2 (n_feature,);</span>
<span class="sd">            Y_2 (float): Input label-2;</span>

<span class="sd">        Returns:</span>
<span class="sd">            Float: d((X_1, Y_1), (X_2, Y_2))</span>
<span class="sd">            </span>
<span class="sd">        Raises:</span>
<span class="sd">            WassersteinDROError: If the dimensions of two input feature are different.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">X_1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">X_2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="n">WassersteinDROError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;two input feature dimensions are different.&quot;</span><span class="p">)</span>
        <span class="c1"># if Y_1 != Y_2 and self.kappa != &#39;inf&#39;:</span>
        <span class="c1">#     warnings.warn(&quot;Despite labels are different, we do not count their difference since we do not allow change in Y.&quot;)</span>

        <span class="n">component_X</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">sqrtm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_matrix</span><span class="p">)</span> <span class="o">@</span> <span class="p">(</span><span class="n">X_1</span> <span class="o">-</span> <span class="n">X_2</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;ols&#39;</span><span class="p">:</span>
            <span class="n">component_X</span> <span class="o">=</span> <span class="n">component_X</span> <span class="o">**</span> <span class="mi">2</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">!=</span> <span class="s1">&#39;inf&#39;</span><span class="p">:</span>
            <span class="n">component_Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">*</span> <span class="n">cp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Y_1</span> <span class="o">-</span> <span class="n">Y_2</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># change of Y is not allowed</span>
            <span class="n">component_Y</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">component_X</span> <span class="o">+</span> <span class="n">component_Y</span></div>
        

<div class="viewcode-block" id="WassersteinDRO.lipschitz_norm"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDRO.lipschitz_norm">[docs]</a>    <span class="k">def</span> <span class="nf">lipschitz_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computing the Lipschitz norm of the loss function</span>

<span class="sd">        Returns:</span>
<span class="sd">            Float: the size of the Lipschitz norm of the loss function</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;svm&#39;</span><span class="p">,</span> <span class="s1">&#39;logistic&#39;</span><span class="p">,</span> <span class="s1">&#39;lad&#39;</span><span class="p">]:</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span></div>
        

<div class="viewcode-block" id="WassersteinDRO.worst_distribution"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDRO.worst_distribution">[docs]</a>    <span class="k">def</span> <span class="nf">worst_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">compute_type</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Compute the worst-case distribution based on Wasserstein Distance</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            X (np.ndarray): Input feature matrix with shape (n_samples, n_features).</span>
<span class="sd">            y (np.ndarray): Target vector with shape (n_samples,).</span>
<span class="sd">            compute_type (int): type of computing the worst case distribution, only suitable for &#39;lad&#39;, &#39;svm&#39;, &#39;logistic&#39;. 1 refers to computing via [1] (which is asymptotically, not exact and not exact and cannot do when kappa = \infty), 2 refers to computing via [2].</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: Dictionary containing &#39;sample_pts&#39; and &#39;weight&#39; keys for worst-case distribution.</span>

<span class="sd">        Raises:</span>
<span class="sd">            Warnings: when ols is set for the compute_type == 1</span>
<span class="sd">            WassersteinDROError: the compute_type == 1 does not support kappa = infty.</span>

<span class="sd">        Reference of Worst-case Distribution:</span>
<span class="sd">        [1] SVM / Logistic / LAD: Theorem 20 (ii) in https://jmlr.org/papers/volume20/17-633/17-633.pdf, where eta is the theta in eq(27) and gamma = 0 in that equation.</span>
<span class="sd">        [2] In all cases, we use a reduced dual case (e.g., Remark 5.2 of https://arxiv.org/pdf/2308.05414) to compute their worst-case distribution.</span>
<span class="sd">        [3] General Worst-case Distributions can be found in: https://pubsonline.informs.org/doi/abs/10.1287/moor.2022.1275, where norm_theta is lambda* here.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;ols&#39;</span> <span class="ow">and</span> <span class="n">compute_type</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;OLS does not support the corresponding computation method.&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">==</span> <span class="s1">&#39;inf&#39;</span> <span class="ow">and</span> <span class="n">compute_type</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">WassersteinDROError</span><span class="p">(</span><span class="s2">&quot;The corresponding computation method do not support kappa = infty!&quot;</span><span class="p">)</span>
        
        <span class="n">sample_size</span><span class="p">,</span> <span class="n">__</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>


        <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">dual_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">!=</span> <span class="s1">&#39;inf&#39;</span><span class="p">:</span>
            <span class="n">dual_norm</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dual_norm</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">norm_theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_inv_transform</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="nb">ord</span> <span class="o">=</span> <span class="n">dual_norm</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">compute_type</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;ols&#39;</span><span class="p">:</span>
                <span class="n">dual_norm_parameter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_inv_transform</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">dual_norm</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
                <span class="n">new_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">sample_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sample_size</span><span class="p">):</span>
                    <span class="n">var_x</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
                    <span class="n">var_y</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">()</span>
                    <span class="n">obj</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">@</span> <span class="n">var_x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">dual_norm_parameter</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance_compute</span><span class="p">(</span><span class="n">var_x</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">var_y</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">problem</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Problem</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">Maximize</span><span class="p">(</span><span class="n">obj</span><span class="p">))</span>
                    <span class="n">problem</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">)</span>
                    <span class="n">new_X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">var_x</span><span class="o">.</span><span class="n">value</span>
                <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;sample_pts&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">new_X</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="s1">&#39;weight&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">sample_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">sample_size</span><span class="p">}</span>

            <span class="k">else</span><span class="p">:</span> <span class="c1"># linear classification or regression with Lipschitz norm</span>
                <span class="c1"># we denote the following case when we do not change Y.</span>
                <span class="n">new_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">sample_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">))</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;svm&#39;</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sample_size</span><span class="p">):</span>
                        <span class="n">var_x</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
                        <span class="n">var_y</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">()</span>
                        <span class="n">obj</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">var_x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">-</span> <span class="n">norm_theta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance_compute</span><span class="p">(</span><span class="n">var_x</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">var_y</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="n">problem</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Problem</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">Maximize</span><span class="p">(</span><span class="n">obj</span><span class="p">))</span>
                        <span class="n">problem</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">)</span>
                        
                        <span class="k">if</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">var_x</span><span class="o">.</span><span class="n">value</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="n">new_X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">new_X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">var_x</span><span class="o">.</span><span class="n">value</span>
                    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;sample_pts&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">new_X</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="s1">&#39;weight&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">sample_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">sample_size</span><span class="p">}</span>

                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;lad&#39;</span><span class="p">,</span> <span class="s1">&#39;logistic&#39;</span><span class="p">]:</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sample_size</span><span class="p">):</span>
                        <span class="n">var_x</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
                        <span class="n">var_y</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">()</span>
                        <span class="n">obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cvx_loss</span><span class="p">(</span><span class="n">var_x</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">norm_theta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance_compute</span><span class="p">(</span><span class="n">var_x</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">var_y</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="n">problem</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Problem</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">Maximize</span><span class="p">(</span><span class="n">obj</span><span class="p">))</span>
                        <span class="n">problem</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">)</span>
                        <span class="n">new_X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">var_x</span><span class="o">.</span><span class="n">value</span>
                    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;sample_pts&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">new_X</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="s1">&#39;weight&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">sample_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">sample_size</span><span class="p">}</span>

                    
            
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># in the following cases, we take gamma = 1 / sample_size since we want the asymptotic with respect to n</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;svm&#39;</span><span class="p">,</span> <span class="s1">&#39;logistic&#39;</span><span class="p">]:</span>
            <span class="c1"># Theorem 20 in https://jmlr.org/papers/volume20/17-633/17-633.pdf, where eta refers to theta in their equation, and eta_gamma refers to eta(\gamma)</span>
                <span class="n">gamma</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">sample_size</span>
                <span class="n">eta</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">nonneg</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
                <span class="n">alpha</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">sample_size</span><span class="p">,</span> <span class="n">nonneg</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

                <span class="c1"># svm / logistic L = 1</span>
                <span class="n">dual_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lipschitz_norm</span><span class="p">()</span> <span class="o">*</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">norm_theta</span> <span class="o">+</span> <span class="n">cp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)))</span> <span class="o">/</span> <span class="n">sample_size</span> <span class="o">+</span> <span class="n">cp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">-</span><span class="n">y</span><span class="p">)))</span> <span class="o">/</span> <span class="n">sample_size</span>
                <span class="n">cons</span> <span class="o">=</span> <span class="p">[</span><span class="n">alpha</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">eta</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">*</span> <span class="n">cp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="n">sample_size</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">-</span> <span class="n">gamma</span><span class="p">]</span>
                <span class="n">problem</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Problem</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">Maximize</span><span class="p">(</span><span class="n">dual_loss</span><span class="p">),</span> <span class="n">cons</span><span class="p">)</span>
                <span class="n">problem</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">)</span>
                <span class="n">eta_gamma</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">/</span> <span class="p">(</span><span class="n">eta</span><span class="o">.</span><span class="n">value</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">.</span><span class="n">value</span><span class="p">)</span> <span class="o">/</span> <span class="n">sample_size</span><span class="p">,</span> <span class="n">alpha</span><span class="o">.</span><span class="n">value</span> <span class="o">/</span> <span class="n">sample_size</span><span class="p">))</span>
                <span class="n">weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">weight</span><span class="p">,</span> <span class="n">eta_gamma</span> <span class="o">/</span> <span class="n">sample_size</span><span class="p">))</span>
                <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">eta_gamma</span><span class="p">)</span>
                <span class="n">weight</span><span class="p">[</span><span class="n">sample_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="n">sample_size</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">eta_gamma</span><span class="p">)</span>
                <span class="c1"># solve the following perturbation problem</span>
                <span class="n">X_star</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
                <span class="n">cons</span> <span class="o">=</span> <span class="p">[</span><span class="n">cp</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">sqrtm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_matrix</span><span class="p">)</span> <span class="o">@</span> <span class="n">X_star</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">]</span>
                <span class="n">problem</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Problem</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">Maximize</span><span class="p">(</span><span class="n">X_star</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">),</span> <span class="n">cons</span><span class="p">)</span>
                <span class="n">problem</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">)</span>

                <span class="n">new_X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">X_star</span><span class="o">.</span><span class="n">value</span> <span class="o">*</span> <span class="n">sample_size</span> <span class="o">*</span> <span class="n">eta</span><span class="o">.</span><span class="n">value</span> <span class="o">/</span> <span class="n">eta_gamma</span>
                <span class="n">new_y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">))</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">new_X</span><span class="p">))</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">y</span><span class="p">,</span> <span class="o">-</span><span class="n">y</span><span class="p">))</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">y</span><span class="p">,</span> <span class="n">new_y</span><span class="p">))</span>
                <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;sample_pts&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="s1">&#39;weight&#39;</span><span class="p">:</span> <span class="n">weight</span><span class="p">}</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;lad&#39;</span><span class="p">:</span>
            <span class="c1"># Theorem 9 in https://jmlr.org/papers/volume20/17-633/17-633.pdf</span>
                <span class="n">gamma</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">sample_size</span>
                <span class="n">weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">sample_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">sample_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">sample_size</span>
                <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">gamma</span><span class="p">)</span> <span class="o">/</span> <span class="n">sample_size</span>
                <span class="n">weight</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">/</span> <span class="n">sample_size</span>
                <span class="c1"># solve the following perturbation problem</span>
                <span class="n">X_star</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
                <span class="n">y_star</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">()</span>
                <span class="n">cons</span> <span class="o">=</span> <span class="p">[</span><span class="n">cp</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">sqrtm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_matrix</span><span class="p">)</span> <span class="o">@</span> <span class="n">X_star</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">*</span> <span class="n">cp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y_star</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">]</span>
                <span class="n">dual_loss</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">X_star</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_star</span>
                <span class="n">problem</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Problem</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">Maximize</span><span class="p">(</span><span class="n">dual_loss</span><span class="p">),</span> <span class="n">cons</span><span class="p">)</span>
                <span class="n">problem</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">)</span>
                <span class="n">new_X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">*</span> <span class="n">sample_size</span> <span class="o">/</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">X_star</span><span class="o">.</span><span class="n">value</span>
                <span class="n">new_y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">*</span> <span class="n">sample_size</span> <span class="o">/</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">y_star</span><span class="o">.</span><span class="n">value</span>
                <span class="n">worst_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">new_X</span><span class="p">))</span>
                <span class="n">worst_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">y</span><span class="p">,</span> <span class="n">new_y</span><span class="p">))</span>
                <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;sample_pts&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">worst_X</span><span class="p">,</span> <span class="n">worst_y</span><span class="p">],</span> <span class="s1">&#39;weight&#39;</span><span class="p">:</span> <span class="n">weight</span><span class="p">}</span></div></div>


<div class="viewcode-block" id="WassersteinDROSatisificingError"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.WassersteinDROSatisificingError">[docs]</a><span class="k">class</span> <span class="nc">WassersteinDROSatisificingError</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base exception class for errors in Wasserstein DRO (Robust Satisficing) model.&quot;&quot;&quot;</span>
    <span class="k">pass</span></div>


<div class="viewcode-block" id="Wasserstein_DRO_satisficing"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing">[docs]</a><span class="k">class</span> <span class="nc">Wasserstein_DRO_satisficing</span><span class="p">(</span><span class="n">BaseLinearDRO</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Robust satisficing version of Wasserstein DRO</span>

<span class="sd">    This model minimizes the subject to (approximated version) of the robust satisficing constraint of Wasserstein DRO. The Wasserstein Distance is defined as the minimum probability coupling of two distributions for the distance metric: </span>
<span class="sd">    d((X_1, Y_1), (X_2, Y_2)) = (abs(cost_matrix^{1/2} @ (X_1 - X_2))_p)^{square} + kappa abs(Y_1 - Y_2).</span>

<span class="sd">    Attributes:</span>
<span class="sd">        input_dim (int): Dimensionality of the input features.</span>
<span class="sd">        model_type (str, default = &#39;svm&#39;): Model type indicator (&#39;svm&#39; for SVM, &#39;logistic&#39; for Logistic Regression, &#39;ols&#39; for Linear Regression for OLS, &#39;lad&#39; for Linear Regression for LAD).</span>
<span class="sd">        fit_intercept (bool, default = True): Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).</span>
<span class="sd">        solver (str, default = &#39;MOSEK&#39;): Optimization solver to solve the problem, default = &#39;MOSEK&#39;</span>
<span class="sd">        target ratio (float): target ratio (required to be &gt;=1, against the empirical objective).</span>
<span class="sd"> </span>
<span class="sd">    Reference: &lt;https://pubsonline.informs.org/doi/10.1287/opre.2021.2238&gt;</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">model_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">solver</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;MOSEK&#39;</span><span class="p">):</span>
        <span class="n">BaseLinearDRO</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">model_type</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="p">,</span> <span class="n">solver</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cost_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cost_inv_transform</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sqrtm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_matrix</span><span class="p">))</span>
        <span class="c1"># target that the robust error need to achieve</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_ratio</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mf">0.8</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">=</span> <span class="mi">1</span>

<div class="viewcode-block" id="Wasserstein_DRO_satisficing.update"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.update">[docs]</a>    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="s1">&#39;cost_matrix&#39;</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cost_matrix</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;cost_matrix&#39;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cost_inv_transform</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sqrtm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_matrix</span><span class="p">))</span>
        <span class="k">if</span> <span class="s1">&#39;target_ratio&#39;</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">assert</span> <span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;target_ratio&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target_ratio</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;target_ratio&#39;</span><span class="p">]</span>
        <span class="c1"># the following two are only used in SVM-wasserstein</span>
        <span class="k">if</span> <span class="s1">&#39;p&#39;</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;p&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="s1">&#39;kappa&#39;</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;kappa&#39;</span><span class="p">]</span></div>
    
<div class="viewcode-block" id="Wasserstein_DRO_satisficing.fit"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">dual_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">!=</span> <span class="s1">&#39;inf&#39;</span><span class="p">:</span>
            <span class="n">dual_norm</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dual_norm</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">sample_size</span><span class="p">,</span> <span class="n">__</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">empirical_rmse</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_oracle</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">TGT</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_ratio</span> <span class="o">*</span> <span class="n">empirical_rmse</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">cons</span> <span class="o">=</span> <span class="p">[</span><span class="n">TGT</span> <span class="o">&gt;=</span> <span class="n">cp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cvx_loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span> <span class="o">*</span> <span class="n">sample_size</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;lad&#39;</span><span class="p">:</span>
            <span class="n">obj</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">theta</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dual_norm</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;ols&#39;</span><span class="p">,</span> <span class="s1">&#39;svm&#39;</span><span class="p">,</span> <span class="s1">&#39;logistic&#39;</span><span class="p">]:</span>
            <span class="n">obj</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">dual_norm</span><span class="p">)</span>

        <span class="n">problem</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Problem</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">Minimize</span><span class="p">(</span><span class="n">obj</span><span class="p">),</span> <span class="n">cons</span><span class="p">)</span>
        <span class="n">problem</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">value</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">value</span>

        <span class="n">model_params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">model_params</span><span class="p">[</span><span class="s2">&quot;theta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">model_params</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
    
        <span class="k">return</span> <span class="n">model_params</span></div>



<div class="viewcode-block" id="Wasserstein_DRO_satisficing.fit_depreciate"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.fit_depreciate">[docs]</a>    <span class="k">def</span> <span class="nf">fit_depreciate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Find the best epsilon that matches the desired robust objective via bisection (depreciated)</span>

<span class="sd">        Args:</span>
<span class="sd">            X (np.ndarray): Input feature matrix with shape (n_samples, n_features).</span>
<span class="sd">            y (np.ndarray): Target vector with shape (n_samples,).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: Model parameters dictionary with &#39;theta&#39; key.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The bisection search is depreciated for Robust Satisficing Wasserstein DRO.&quot;</span><span class="p">)</span>
        <span class="n">iter_num</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="c1"># determine the empirical obj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">empirical_rmse</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_oracle</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">TGT</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_ratio</span> <span class="o">*</span> <span class="n">empirical_rmse</span>
        <span class="c1"># print(&#39;tgt&#39;, TGT)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="k">assert</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_oracle</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">TGT</span><span class="p">)</span>
        <span class="n">eps_lower</span><span class="p">,</span> <span class="n">eps_upper</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span>      
        <span class="c1"># binary search and find the maximum eps, such that RMSE + eps theta &lt;= tau  </span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iter_num</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="p">(</span><span class="n">eps_lower</span> <span class="o">+</span> <span class="n">eps_upper</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_oracle</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">TGT</span><span class="p">:</span>
                <span class="n">eps_upper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">eps_lower</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span>
        
        <span class="n">model_params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">model_params</span><span class="p">[</span><span class="s2">&quot;theta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">model_params</span></div>
    
<div class="viewcode-block" id="Wasserstein_DRO_satisficing.penalization"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.penalization">[docs]</a>    <span class="k">def</span> <span class="nf">penalization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Module for computing the regularization part in the standard Wasserstein DRO problem.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta (np.ndarray): Feature vector with shape (n_feature,).</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            Float: Regularization term part.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">dual_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dual_norm</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;ols&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">cp</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_inv_transform</span> <span class="o">@</span> <span class="n">theta</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;svm&#39;</span><span class="p">,</span> <span class="s1">&#39;logistic&#39;</span><span class="p">]:</span>
            <span class="k">return</span> <span class="n">cp</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_inv_transform</span> <span class="o">@</span> <span class="n">theta</span><span class="p">,</span> <span class="n">dual_norm</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;lad&#39;</span><span class="p">:</span>
            <span class="c1"># the dual of the \|\theta, -1\|_dual_norm penalization</span>
            <span class="k">return</span> <span class="n">cp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_inv_transform</span> <span class="o">@</span> <span class="n">theta</span><span class="p">,</span> <span class="n">dual_norm</span><span class="p">),</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span><span class="p">)</span></div>
            

<div class="viewcode-block" id="Wasserstein_DRO_satisficing.fit_oracle"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.fit_oracle">[docs]</a>    <span class="k">def</span> <span class="nf">fit_oracle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Depreciated, find the optimal thet given the ambiguity constraint.</span>

<span class="sd">        Args:</span>
<span class="sd">            X (np.ndarray): Input feature matrix with shape (n_samples, n_features).</span>
<span class="sd">            y (np.ndarray): Target vector with shape (n_samples,).</span>

<span class="sd">        Returns:</span>
<span class="sd">            float: robust objective value</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">sample_size</span><span class="p">,</span> <span class="n">feature_size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="n">feature_size</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">WassersteinDROError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected input with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="si">}</span><span class="s2"> features, got </span><span class="si">{</span><span class="n">feature_size</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sample_size</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="n">WassersteinDROError</span><span class="p">(</span><span class="s2">&quot;Input X and target y must have the same number of samples.&quot;</span><span class="p">)</span>


        <span class="n">theta</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>


        <span class="n">lamb_da</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">()</span>
        <span class="n">cons</span> <span class="o">=</span> <span class="p">[</span><span class="n">lamb_da</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalization</span><span class="p">(</span><span class="n">theta</span><span class="p">)]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;ols&#39;</span><span class="p">:</span>
            <span class="n">final_loss</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">theta</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sample_size</span><span class="p">)</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">lamb_da</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;svm&#39;</span><span class="p">,</span> <span class="s1">&#39;logistic&#39;</span><span class="p">]:</span>
                <span class="n">s</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">sample_size</span><span class="p">)</span>
                <span class="n">cons</span> <span class="o">+=</span> <span class="p">[</span><span class="n">s</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cvx_loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">b</span><span class="p">)]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">!=</span> <span class="s1">&#39;inf&#39;</span><span class="p">:</span>
                    <span class="n">cons</span> <span class="o">+=</span> <span class="p">[</span><span class="n">s</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cvx_loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">-</span><span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">lamb_da</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span><span class="p">]</span>
                <span class="n">final_loss</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="n">sample_size</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">*</span> <span class="n">lamb_da</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># model type == &#39;lad&#39; for general p.</span>
                <span class="n">final_loss</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cvx_loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span> <span class="o">/</span> <span class="n">sample_size</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">*</span> <span class="n">lamb_da</span>

        <span class="n">problem</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Problem</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">Minimize</span><span class="p">(</span><span class="n">final_loss</span><span class="p">),</span> <span class="n">cons</span><span class="p">)</span>

        <span class="n">problem</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">value</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>

        <span class="k">return</span> <span class="n">problem</span><span class="o">.</span><span class="n">value</span></div>
        
    
<div class="viewcode-block" id="Wasserstein_DRO_satisficing.worst_distribution"><a class="viewcode-back" href="../../../../api/linear_model.html#dro.src.linear_model.Wasserstein_DRO.Wasserstein_DRO_satisficing.worst_distribution">[docs]</a>    <span class="k">def</span> <span class="nf">worst_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;We do not compute worst case distribution for robust satisficing model since the distribution constraint is set to be held for any distribution.&quot;</span><span class="p">)</span></div></div>


        <span class="c1"># REQUIRED TO BE CALLED after solving the DRO problem</span>
        <span class="c1"># return a dict {&quot;sample_pts&quot;: [np.array([pts_num, input_dim]), np.array(pts_num)], &#39;weight&#39;: np.array(pts_num)}</span>

        <span class="c1"># if self.is_regression == 1 or self.is_regression == 2:</span>
        <span class="c1">#     return NotImplementedError</span>
        <span class="c1"># else:</span>
        <span class="c1">#     sample_size, __ = X.shape</span>
        <span class="c1">#     if self.p == 1:</span>
        <span class="c1">#         dual_norm = np.inf</span>
        <span class="c1">#     else:</span>
        <span class="c1">#         dual_norm = 1 / (1 - 1 / self.p)</span>
        <span class="c1">#     norm_theta = np.linalg.norm(self.theta, ord = dual_norm)</span>
        <span class="c1">#     if self.kappa == 1000000:</span>
        <span class="c1">#     # not change y, we directly consider RMK 5.2 in https://arxiv.org/pdf/2308.05414.pdf, here norm_theta is lambda* there.</span>
        <span class="c1">#         new_X = np.zeros((sample_size, self.input_dim))</span>
        <span class="c1">#         for i in range(sample_size):</span>
        <span class="c1">#             var_x = cp.Variable(self.input_dim)</span>
        <span class="c1">#             obj = 1 - y[i] * var_x @ self.theta - norm_theta * cp.sum_squares(var_x - X[i])</span>
        <span class="c1">#             problem = cp.Problem(cp.Maximize(obj))</span>
        <span class="c1">#             problem.solve(solver = self.solver)</span>
                    
        <span class="c1">#             if 1 - y[i] * var_x.value @ self.theta &lt; 0:</span>
        <span class="c1">#                 new_X[i] = X[i]</span>
        <span class="c1">#             else:</span>
        <span class="c1">#                 new_X[i] = var_x.value</span>
        <span class="c1">#         return {&#39;sample_pts&#39;: [new_X, y], &#39;weight&#39;: np.ones(sample_size) / sample_size}</span>
            
        <span class="c1">#     else:</span>
        <span class="c1">#         # for general situations if we can change y, we apply Theorem 20 (ii) in https://jmlr.org/papers/volume20/17-633/17-633.pdf (SVM / logistic loss)</span>
        <span class="c1">#         #eta is the theta in eq(27)</span>
        <span class="c1">#         y_flip = -y</span>
        <span class="c1">#         eta = cp.Variable(nonneg = True)</span>
        <span class="c1">#         alpha = cp.Variable(sample_size, nonneg = True)</span>

        <span class="c1">#         # svm / logistic L = 1</span>
        <span class="c1">#         L = 1</span>
        <span class="c1">#         dual_loss = L * eta * norm_theta + cp.sum(cp.multiply(1 - alpha, self.loss(X, y))) / sample_size + cp.sum(cp.multiply(alpha, self.loss(X, y_flip))) / sample_size</span>
        <span class="c1">#         cons = [alpha &lt;= 1, eta + self.kappa * cp.sum(alpha) / sample_size == self.eps]</span>
        <span class="c1">#         problem = cp.Problem(cp.Maximize(dual_loss), cons)</span>
        <span class="c1">#         problem.solve(solver = self.solver)</span>
        <span class="c1">#         weight = np.concatenate(((1 - alpha.value) / sample_size, alpha.value / sample_size))</span>
        <span class="c1">#         X = np.concatenate((X, X))</span>
        <span class="c1">#         y = np.concatenate((y, y_flip))</span>
        <span class="c1">#         return {&#39;sample_pts&#39;: [X, y], &#39;weight&#39;: weight}</span>

        






        

        
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Jiashuo Liu, Tianyu Wang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>